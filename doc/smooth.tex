\input{header.tex}

\begin{document}

\centerline{\bf\Large Smooth Emulator and Simplex Sampling}
\centerline{Scott Pratt}
\centerline{Michigan State University}

\section{Smooth Emulator}

The choice of model emulators, $E(\theta)$, depends on the prior understanding of the model being emulated, $M(\theta)$. If one knows that  a function is linear, then a linear fit is clearly the best fit. Whereas to reproduce lumpy features, where the lumps have a characteristic length scale, Gaussian process emulators are often an excellent choice. The quality of an emulator can be assessed through the following criteria:
\begin{itemize}
  \item $E(\theta_t)=M(\theta_t)$ at the training points, $\theta_t$. 
  \item The emulator should reasonably reproduce the model away from the training points. This should hold true for either interpolation or extrapolation.
  \item The emulator should accurately represent its uncertainty
  \item A minimal number of training points should be needed
  \item The method should easily adjust to larger numbers of parameters, $\theta_i,~i=1\cdots N$.
  \item The emulator should be able to account for noisy models
  \item Training and running the emulator should not be numerically intensive
\end{itemize}

Here the goal is to focus on a particular class of functions. Those functions are {\it smooth}. Smoothness is a prior knowledge of the function. It is an expectation that the linear terms of the function are likely to provide more variance than the quadratic contributions, which are in turn likely to be more important than the cubic corrections, and so on. 

To that end the following form for $E(\theta)$ is proposed,
\begin{align*}\eqnumber\label{eq:emudef}
E(\vec{\theta})&=f(|\vec{\theta}|)\sum_{\vec{n}, {\rm s.t.}\sum_in_i\le K_{\rm max}} d_{\langle n_i\rangle}
A_{\vec{n}}
\left(\frac{\theta_1}{\Lambda}\right)^{n_1}\left(\frac{\theta_2}{\Lambda}\right)^{n_2}\cdots 
\left(\frac{\theta_N}{\Lambda}\right)^{n_N}.
\end{align*}
Each term has a rank $K_{\vec{n}}=n_1+n_2+\cdots n_N$. All terms are included up to a given rank. The coefficients $A$ are stochastically distributed. For each combination $\vec{n}$, the probability is weighted by 
\begin{align*}\eqnumber
p(A_{\vec{n}})\sim e^{-A_{\vec{n}}^2/2\sigma_E^2}, 
\end{align*}
where both the $A$ coefficients and $\sigma_E$ will also be allowed to vary. The parameter $\Lambda$ will be referred to as the smoothness parameter. Here, we assume that all parameters have a similar range, or order unity, e.g. $-1<\theta_i<1$. Thus, the relative importance of each term Eq. (\ref{eq:emudef}) falls with increasing rank, $K=\sum_in_i$, as $(1/\Lambda)^K$. The smoothness parameter is fixed by prior knowledge, i.e. one chooses higher values of $\Lambda$ if one believes the function to be basically linear.

The parameters $d_{\vec{n}}$ and the function $f(|\vec{\theta}|^2)$ will be chosen to constrain the variance of $E$, as the $A$ coefficients are varied to be independent of the position in parameter space. Clearly, at $|\vec{\theta}|=0$ the only term in Eq. (\ref{eq:emudef}) that contributes is the one $K=0$ term. Averaging over $A$ coefficients, which can be either positive or negative with equal probability,
\begin{align*}\eqnumber
\langle E(\vec{\theta})\rangle=0,
\end{align*}
where the averaging refers to an average over the $A$ coefficients. At the origin, $|\vec{\theta}|=0$, the variance of $E$ is
\begin{align*}\eqnumber
\langle E(\theta_1=\theta_2=\cdots\theta_N=0)^2\rangle=f^2(\vec{\theta}=0)d^2_{n_i=0}\sigma_E^2.
\end{align*}
Choosing $f(0)=1$ and $d_{n_i=0}=1$, the variance of $E$ is indeed $\sigma_E^2$. The functions $d_c$ and $f(|\vec{\theta}|^2)$ are further constrained by the requirement that the variance is independent of $\vec{\theta}$. 
\begin{align*}\eqnumber
\langle E^2(\vec{\theta})\rangle &=
f^2(|\vec{\theta}|)\sigma_E^2\sum_{\vec{n}} \frac{1}{\Lambda^{2K_{\vec{n}}}}d_{\vec{n}}^2
\left(\frac{\theta_1^{2n_1}}{\Lambda^2}\right)\left(\frac{\theta_2^{2n_2}}{\Lambda^2}\right)\cdots 
\left(\frac{\theta_N^{2n_N}}{\Lambda^2}\right).
\end{align*}
If $\langle E^2\rangle$ is to be independent of the direction of $\vec{\theta}$, the sum above must be a function of $|\vec{\theta}|^2$ only. This requires the net contribution from each rank, $K$ to be proportional to  $|\vec{\theta}|^{2K}$ multiplied by some function of $K_c$. Using the fact that
\begin{align*}\eqnumber
(\sum_{i=1}^N \theta_i^2)^K&=\sum_{n_1\cdots n_N,s.t. \sum_i n_i=K}\frac{K!}{n_1!\cdots n_N!}\theta_1^{2n_1}\cdots\theta_N^{2n_N},
\end{align*}
one can see that if the sum is to depend only on the norm of $\vec{\theta}$,
\begin{align*}\eqnumber
d_{\vec{n}}^2&=\beta(K_{\vec{n}})\frac{K_{\vec{n}}!}{n_1!n_2!\cdots n_N!},
\end{align*}
where $\beta(K)$ is any function of the rank. One can perform the sums for all terms of a given rank, and obtain
\begin{align*}\eqnumber
\langle E^2(\vec{\theta})\rangle &=\sigma_E^2
f(|\vec{\theta}|)\sum_K\beta(K)\left(\frac{|\vec{\theta}|^2}{\Lambda^2}\right)^{K}.
\end{align*}
If the sum is to equal $\sigma_E^2$ and be independent of $|\vec{\theta}|$,
\begin{align*}\eqnumber\label{eq:fform}
f^2(|\vec{\theta}|)&=\left[\sum_{K=0}^{K_{\rm max}}\beta(K)\left(\frac{|\vec{\theta}|^2}{\Lambda^2}\right)^{K}\right]^{-1}.
\end{align*}
The function $\beta$ could be any function of the rank $K$, but one would like the sum to be finite for any $|\vec{\theta}|$, even when $|\vec{\theta}|>\Lambda$. An obvious choice is 
\begin{align*}\eqnumber
\beta(K)&=\frac{1}{K!}.
\end{align*}
This form will be assumed hereafter. In practice, a finite number of coefficients are included, all those of rank $K_{\rm max}$ or less. If one were to take the limit $K_{\rm max}\rightarrow\infty$ the prefactor $f(|\vec{\theta}|)$ would simply be an exponential,
\begin{align*}\eqnumber
f(|\vec{\theta}|)|_{K_{\rm max}\rightarrow\infty}=\exp\left(-\frac{|\vec{\theta}|^2}{2\Lambda^2}\right).
\end{align*}
This gives us our final form for the emulator,
\begin{align*}\eqnumber\label{eq:smooth}
E(\vec{\theta})&=f(|\vec{\theta}|^2)\sum_{\vec{n}~{\rm s.t.}K_{\vec{n}}\le K_{\rm max}} \left[n_1!n_1!\cdots n_N!\right]^{-1/2}A_{\vec{n}}
\left(\frac{\theta_1}{\Lambda}\right)^{n_1}
\cdots \left(\frac{\theta_N}{\Lambda}\right)^{n_N},
\end{align*}
where the sum over $c$ is the sum overall possible combinations of $n_{1c}\cdots n_{Nc}$ that sum to $K_{\rm max}$ or less, and where the function $f(|\vec{\theta}|)$ is given by Eq. (\ref{eq:fform}).

\subsection{Correlations}

The form for the emulator in Eq. (\ref{eq:smooth}) results in correlations that fall with relative distance. The correlations have a particularly simple form in the $K_{\rm max}\rightarrow \infty$ limit. In that limit
\begin{align*}\eqnumber
\langle E(\vec{\theta})E(\vec{\theta}')\rangle&=e^{-|\vec{\theta}'|^2/2\Lambda^2}e^{-|\vec{\theta}|^2/2\Lambda^2}
\sum_{\vec{n}}\frac{1}{n_1!\cdots n_N!}\langle A_{\vec{n}}^2 
\left(\frac{\theta_1\theta'_1}{\Lambda^2}\right)^{n_1}\cdots
\left(\frac{\theta_N\theta'_N}{\Lambda^2}\right)^{n_N}\\
&=\sigma_E^2e^{-|\vec{\theta}'|^2/2\Lambda^2}e^{-|\vec{\theta}|^2/2\Lambda^2}
\sum_K\frac{1}{K!}\left(\frac{\vec{\theta}\cdot\vec{\theta}'}{\Lambda^2}\right)^K\\
&=\sigma_E^2e^{-|\vec{\theta}-\vec{\theta}'|^2/2\Lambda^2}.
\end{align*}
The smoothness parameter also sets the correlation length.


\end{document}
