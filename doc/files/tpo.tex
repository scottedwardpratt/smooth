\documentclass[UserManual.tex]{subfiles}
\begin{document}
\setcounter{section}{2}

\section{Generating Training Points with {\it Smooth Emulator}'s training point optimizer}\label{sec:tpo}

\subsection{Summary}
The executable {\tt trainingpoint\_optiizer}, found in {\tt \$\{MY\_LOCAL\}/bin/} produces a list of points in the $n-$dimensional model-parameter space to be used for training an emulator. The points are chosen to minimize the estimate of the expected emulator uncertainty integrated over the model-parameter prior. Because the full model values are not determined at the time the training points are chosen, the estimate is based on the User's best guess for the convergence hyper-parameter $\Lambda$. 

The program first reads a simple text file that provides the names of model parameters, the range of their prior distribution and their expected relative sensitivities, {\tt smooth\_data/Info/prior\_info.txt}. Options for {\tt trainingpoint\_optimizer} are taken from a separate text file, {\tt smooth\_data/Options/tpo\_options.txt}. This enables the User to make choices, such as which algorithm to apply when generating the training points. For some algorithms, {\tt trainingpoint\_optimizer} chooses the number of training points, whereas for other options the User chooses the number. Before the emulator is built and tuned, the User is free to run the full model at additional points, or to use their own method to generate training points and forego {\tt trainingpoint\_optimizer} entirely.

\subsection{Specifying Model Parameters and Priors}\label{subsec:priorinfo}
Before proceeding, the executable {\tt trainingpoint\_optimizer} requires information about the parameters, specifically, their ranges. The User enters this information into the file {\tt smooth\_data/Info/prior\_info.txt}, as described in Sec. \ref{sec:installation}. An example of such a file might be
{\tt\begin{verbatim}
# par_name dist_type  xmin/centroid  xmax/Rgauss  SensitivityScale
   par0 gaussian 0 100  1.00000
   par1 gaussian 0 100  1.00000
   par2 uniform  0 100  1.00000
   par3 gaussian 0 100  0.5
   par4 uniform  0 100  0.5
   par5 gaussian 0 100  0.5
\end{verbatim}
}
The first column is the model-parameter name, and the last three parameters describe the range of the parameters, which is usually the prior, assuming the prior is uniform or Gaussian. The second entry for each parameter defines whether the range/prior is {\tt uniform} or {\tt gaussian}. If the prior is {\tt uniform}, the next two numbers specify the lower and upper ranges of the parameter. If the range/prior is {\tt gaussian}, the third entry describes the center of the Gaussian, $x_0$, and the fourth entry describes the Gaussian width, $\sigma_0$, where the prior distribution is $\propto \exp\{-(x-x_0)^2/2\sigma_0^2\}$. {\tt trainingpoint\_optimizer} will read the information to determine the number of parameters. It will then translate and scale the model parameters from $x_i$ to $\theta_i$ so that the $n$ values of $\theta$, $\theta_{1\cdots n}$ assuming each dimension of $\theta$ varies either from -1 to 1, for uniform distributions, or proportional to $e^{-3\theta^2/2}$ for Gaussian distributions. With this choice the individual priors all have the same r.m.s. values once converted to the parameters $\theta_i$.

{\tt training\_point\_optimizer} takes all this information into account when optimizing the selection of points. For example if the sensitivity scale for a given model parameter is small, points will be chose so better capture behavior along the other parameters. Section \ref{sec:theory} provides a brief description of the theoretical basis of the training-point selection and for more detailed description one can read \href{./smoothdraft.pdf}{{\tt smoothdraft.pdf}}.



\subsection{Training Point Optimizer (TPO) Options}
These options are all set in the ascii file {\tt smooth\_data/Options/tpo\_options.txt}, where the path is relative to the analysis directory. An example of the file is:
{\tt
\begin{verbatim}
 #LogFileName               tpolog.txt  # comment out to direct output to screen
 TPO_Method  MCQuadratic  # can be "MC", "MCSphere", "MCSimplex", "MCSimplexPlus1", "MCQuadratic", "MCSphereQuadratic"
 TPO_NTrainingPts    0   # only relevant for OptimizeMethod="MCSphere" or "MC"
 TPO_NMC  10000
 TPO_ALPHA 0.01
 TPO_Include_LAMBDA_Uncertainty true
 TPO_FullModelRunsDirName  smooth_data/FullModelRuns
 #TPO_ReadPoints 0,1,4-7,10,11-13,21
 #TPO_FreezePoints 0,1,4-7,10,11-13,21
\end{verbatim}
}
For each line the first string is the option name and is followed by the value. Both are single strings (without spaces). The \# symbol is used for comments. Each parameter has a default value, which will be used if the parameter is not mentioned in the parameter file.  The paths are provided relative to the analysis directory, i.e. the directory from which you run the {\tt trainingpoint\_optimizer} command. The various options control the behavior as described below.

\subsubsection{{\tt LogFileName}}
If this is left blank, {\it Training Point Optimizer} will write output to the screen. Otherwise it will write output to the named file. Given that {\it Training Point Optimizer} runs in a few seconds, the program is usually run interactively and output is sent to the screen.
\subsubsection{{\tt TPO\_Method} (default is {\tt MC})}
The option {\tt TPO\_Method} sets the training point method and can be set to the following values:
\begin{itemize}\itemsep 0pt
\item {\tt MC}: This is the default. It uses a Monte Carlo procedure to adjust all the training-point locations to optimize the metric defining the best placements. This should be the choice for 
\item {\tt MCSphere}: This constrains all the points, except for one, to be located on the surface of a sphere. The last point is placed in the center. The radius of the sphere is adjusted to optimize the metric.
\item {\tt MCSimplex}: This places all the points at the same radius and equally spaced from each other. The number of training points for this option is automatically set to $N_{\rm train}=N_{\rm par}+1$. Examples of simplexes are: an equilateral triangle for $N_{\rm par}=2$ and a tetrahedron for $N_{\rm par}=3$. If one is choosing the number of training points to correspond to the minimum number required to fix a linear fit, and if the prior is spherically symmetric (e.g. gaussians with the same sensitivity scales), then this option is precisely the optimum one.
\item {\tt MCSimplexPlus1}: The same as above, but adding one point in the center. The number of training points is then set to $N_{\rm train}=N_{\rm par}+2$. 
\item {\tt MCQuadratic}: This is the same as the {\tt MC} option except that the number of training points is fixed at $N_{\rm train}=(N_{\rm par}+1)(N_{\rm par}+2)/2$, which is sufficient to fix all the coefficients in a quadratic expansion.
\item {\tt MCSphereQuadratic}: This is the same as {\tt MCSphere}, except that the number of training points is set to $N_{\rm train}=(N_{\rm par}+1)(N_{\rm par}+2)/2$.
\end{itemize}

\subsubsection{{\tt TPO\_NTraining\_Pts} (default 0)}
For the {\tt MC} and {\tt MCSphere} methods this sets $N_{\rm train}$, the number of training points. Otherwise $N_{\rm train}$ is set to correspond to the method as described above.

\subsubsection{{\tt TPO\_ALPHA} (default 0)}
The value {\tt ALPHA} represents the point-to-point noise, which might vary from one observable to another. But the same training points are used for all observables. Thus, choosing {\tt AlPHA} can be not wholly consistent with those values defined for the observables. Fortunately, because the training points will be chosen far apart, this option has little impact and one can simply set it equal to zero with no significant change to the optimum placement.

\subsubsection{{\tt TPO\_INCLUDE\_LAMBDA\_UNCERTAINTY} (default is {\tt true})}
If set to {\tt true} the optimization accounts for the fact that $\Lambda$ is not known. The procedure will be faster if set to {\tt false}, but that can significantly change the answer in some cases. It is recommended to leave this as {\tt true}.

\subsubsection{{\tt TPO\_FullModelRunsDirName} (default {\tt smooth\_data/FullModelRuns})}
This sets the directory into which the full model output for the training points will be stored. A similar parameter is used by the emulator and is set in {\tt smooth\_data/Options/emulator\_options.txt}, so the User should be careful that if the output from the optimizer is changed in a different location that the emulator also knows where to find it.

\subsubsection{{\tt TPO\_ReadPoints} (default {\tt NONE})}
If set to {\tt NONE}, the MC procedure will be initialized according to latin hyper-cube sampling. If set to {\tt ALL}, the procedure initialize the search using the training point locations described in the {\tt smooth\_data/FullModelRuns/runI/model\_parameters.txt} files. This is useful if one has a better idea of how to initialize the points, or if the point is to be frozen at a specific location as defined by the next option below. If the User wishes to read in a subset of points, the string can list those points which will be read in, while the others will be initialized randomly. An example of the string is {\tt 0-2,5,21-23}.

\subsubsection{{\tt TPO\_FreezePoints} (default {\tt NONE})}
If set to {\tt NONE} all the training points are allowed to vary in the optimization procedure. If set to a string, e.g {\tt 0-2,5,21-23}, the procedure will freeze those points denoted in the string. These points should be a subset of the those points that are read in as described above.
 
\subsection{Running {\tt trainingpoint\_optimizer} to Generate Training Points}

To run {\it Training Point Optimizer}, first make sure the program is compiled. To compile the programs, change into the {MY\_LOCAL/main\_programs/} directory and enter the following command,

\begin{verbatim}
   ${MY_LOCAL}/software% cmake .
   ${MY_LOCAL}/software% make trainingpoint_optimizer
\end{verbatim}

Next, change into your analysis directory and run the program.
{\tt
\begin{verbatim}
   ${MY_ANALYIS}% ${MY_LOCAL}/bin/trainingpoint_optimizer 
\end{verbatim}
}
Here {\tt \$\{MY\_LOCAL\}/bin} is the path to where the User compiles the main programs into executables. 

{\tt trainingpoint\_optimizer} will read parameters from the {\tt smooth\_data/Options/tpo\_options.txt} file and from the {\tt smooth\_data/Info/prior\_info.txt} files. It will then write the information about the training points in the directory {\tt smooth\_data/FullModelRuns/}. Within the directory, a sub-directory will be created for each training point, named {\tt run0/, run1/, run2/}$\cdots$. Within each subdirectory, {\tt trainingpoint\_optimizer} creates a file {\tt runI/model\_parameters.txt} for the I$^{\rm th}$ training point. For example, the {\tt run0/model\_parameters.txt} file might be
{\tt\begin{verbatim}
   NuclearCompressibility     229.08
   ScreeningMass              0.453
   Viscosity                  0.192
\end{verbatim}
}
At this point, it is up to the User to run their full model at each training point and create a file {\tt runI/obs.txt}, which stores values of the observables at those training points as calculated by the full model. 

\subsection{Using Other Choices (other than from {\it Training Point Optimizer}) for Training Points}

If the User desires to use their own procedure for training points, or if the User wishes to augment the {\it Training Point Optimizer} points with additional training points, the User can write their own files {\tt runI/model\_parameters.txt}. The emulator should work fine, though the User should remember that when training the emulator (see Sec. \ref{sec:emulator}) the emulator parameter describing with {\tt runI} directories to apply should be modified.

There is one warning to be issued when choosing training points for tuning {\it Smooth Emulator}. When solving for the Taylor coefficients, the emulator uses the factor of those coefficients (products of $\theta_i$) in the linear algebra routine. However, if those coefficients are not linearly independent, the solution becomes undetermined. For example, if two of the training points were exactly the same, it would fail. 

\end{document}
