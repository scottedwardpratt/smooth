\documentclass[UserManual.tex]{subfiles}
\begin{document}
\setcounter{section}{2}

\section{Generating Training Points with {\it Smooth Emulator}'s training point optimizer}\label{sec:tpo}

\subsection{Summary}
The executable {\tt trainingpoint\_optiizer} produces a list of points in the $n-$dimensional model-parameter space to be used for training an emulator. The points are chosen to minimize the estimate of the expected emulator uncertainty integrated over the model-parameter prior. Because the full model values are not determined at the time the training points are chosen, the estimate is based on the User's best guess for the convergence hyper-parameter $\Lambda$. 

The program first reads a simple text file that provides the names of model parameters, the range of their prior distribution and their expected relative sensitivities, {\tt smooth\_data/Info/prior\_info.txt}. Options for {\tt trainingpoint\_optimizer} are taken from a separate text file, {\tt smooth\_data/Options/tpo\_options.txt}. This enables the User to make choices, such as which algorithm to apply when generating the training points. For some algorithms, {\tt trainingpoint\_optimizer} chooses the number of training points, whereas for other options the User chooses the number. Before the emulator is built and tuned, the User is free to run the full model at additional points, or to use their own method to generate training points and forego {\tt trainingpoint\_optimizer} entirely.

\subsection{Training Point Optimizer (TPO) Options}

If one visits the User's analysis directory, these options are defined in the file  {\tt smooth\_data/Options/tpo\_options.txt}, where the path is relative to the analysis directory. An example of the file is:
{\tt
\begin{verbatim}
 #LogFileName               tpolog.txt  # comment out to direct output to screen
 TPO_Method  MCQuadratic  # can be "MC", "MCSphere", "MCSimplex", "MCSimplexPlus1", "MCQuadratic", "MCSphereQuadratic"
 TPO_NTrainingPts    0   # only relevant for OptimizeMethod="MCSphere" or "MC"
 TPO_NMC  10000
 TPO_ALPHA 0.01
 TPO_INCLUDE_LAMBDA_UNCERTAINTY true
 Smooth_FullModelRunDirName  smooth_data/FullModelRunDirName
 #TPO_ReadPoints 0,1,4-7,10,11-13,21
 #TPO_FreezePoints 0,1,4-7,10,11-13,21
\end{verbatim}
}
For the parameter file, the first string is the parameter name and is followed by the value. Both are single strings (without spaces). The \# symbol is used for comments. Each parameter has a default value, which will be used if the parameter is not mentioned in the parameter file.  {\it Training Point Optimizer} has four User-defined parameters.
\begin{enumerate}\itemsep 0pt
    \item {\bf Simplex\_TrainType}\\
Possible values are ``1'' or ``2''. The default, ``1'', will position points according to a simplex, i.e. in two dimensions this is an equilateral triangle and in three dimensions, it is a tetrahedron. In $n$ dimensions there are $n+1$ points separated at equal distances from one another and centered at the origin. For ``2'', points are added at the half-way points between each vertex of the tetrahedron. The points at the bisection points are scaled to a different radius than those at the vertices. This provides the precise number of training points to exactly determine both the linear and quadratic terms.
\item {\bf Smooth\_FullModelRunDirName}\\
This sets the path to the directory in which the run files will be created. The default name is {\tt smooth\_data/FullModelRuns}, but the User can change this to anything they want. The path is relative to the analysis directory, i.e. the directory from which you run the {\it simplex} command.
\item {\bf LogFileName}\\
If this is left blank, {\it Training Point Optimizer} will write output to the string. Otherwise it will write output to a file. Given that {\it Training Point Optimizer} runs in a few seconds, the program is usually run interactively and output is sent to the screen.
\end{enumerate}

\subsection{Specifying Model Parameters and Priors}\label{subsec:priorinfo}
Before proceeding, the executable {\tt trainingpoint\_optimizer} requires information about the parameters, specifically, their ranges. The User enters this information into the file {\tt smooth\_data/Info/prior\_info.txt}. An example of such a file might be
{\tt\begin{verbatim}
	#LogFileName               tpolog.txt  # comment out to direct output to screen
	TPO_Method  MCQuadratic  # can be "MC", "MCSphere", "MCSimplex", "MCSimplexPlus1",
							 # "MCQuadratic", "MCSphereQuadratic"
	TPO_NTrainingPts    0    # only relevant for OptimizeMethod="MCSphere" or "MC"
	TPO_NMC  10000
	TPO_INCLUDE_LAMBDA_UNCERTAINTY true
	#TPO_ReadPoints 0,1,4-7,10,11-13,21
	#TPO_FreezePoints 0,1,4-7,10,11-13,21
\end{verbatim}
}
The first column is the model-parameter name, and the last three parameters describe the range of the parameters, which is usually the prior, assuming the prior is uniform or Gaussian. The second entry for each parameter defines whether the range/prior is {\tt uniform} or {\tt gaussian}. If the prior is {\tt uniform}, the next two numbers specify the lower and upper ranges of the parameter. If the range/prior is {\tt gaussian}, the third entry describes the center of the Gaussian, $x_0$, and the fourth entry describes the Gaussian width, $\sigma_0$, where the prior distribution is $\propto \exp\{-(x-x_0)^2/2\sigma_0^2\}$. {\tt trainingpoint\_optimizer} will read the information to determine the number of parameters. It will then assign the $n$ points, $\theta_{1\cdots n}$ assuming each dimension of $\theta$ varies from -1 to 1, for uniform distributions, or proportional to $e^{-\theta^2/2}$ for Gaussian distributions. The points $\theta_i$ are each then converted into $x_i$ by scaling and translating the values according to the ranges/priors defined in the {\tt prior\_info.txt} file.

{\tt\begin{verbatim}
	#LogFileName               tpolog.txt  # comment out to direct output to screen
	TPO_Method  MCQuadratic  # can be "MC", "MCSphere", "MCSimplex", "MCSimplexPlus1",
							 # "MCQuadratic", "MCSphereQuadratic"
	TPO_NTrainingPts    0    # only relevant for OptimizeMethod="MCSphere" or "MC"
	TPO_NMC  10000
	TPO_INCLUDE_LAMBDA_UNCERTAINTY true
	#TPO_ReadPoints 0,1,4-7,10,11-13,21
	#TPO_FreezePoints 0,1,4-7,10,11-13,21
\end{verbatim}
}

\subsection{Training Types}

\subsubsection{Type 1}
 Depending on the number of parameters, $n$, the program creates a simplex in $n$ dimensions. This simplex's vertices will be used to generate $N_{\rm train}=n+1$ training points. These points will be scaled by different values so the training points aren't in the same radius. This results in the minimum number of required points for linear fits. Thus, if the model is perfectly linear, this option provides perfect emulation. 
 \subsubsection{Type 2}

 Depending on the number of parameters, the program first creates a simplex in $n$ dimensions. This simplex's vertices will be used to generate new training points there and along the edges. These points will be scaled to be in different radii from the center. This results in the minimum number of required points for quadratic fits. The net number of training points is then $N_{\rm train}=n+1+n(n+1)/2$.  Thus, if the model is perfectly quadratic, this option provides perfect emulation. Rather arbitrarily, the points generated from the midpoints of the original simplex pairs are all pushed out to a large radius, and those from the original simplex are brought somewhat inward.
 
\subsection{Running {\tt trainingpoint\_optimizer} to Generate Training Points}

To run {\it Training Point Optimizer}, first make sure the program is compiled. To compile the programs, change into the {MY\_LOCAL/main\_programs/} directory and enter the following command,

\begin{verbatim}
   ${MY_LOCAL}/software% cmake .
   ${MY_LOCAL}/software% make trainingpoint_optimizer
\end{verbatim}

Next, change into your analysis directory and run the program.
{\tt
\begin{verbatim}
   ${MY_ANALYIS}% ${MY_LOCAL}/bin/trainingpoint_optimizer 
\end{verbatim}
}
Here {\tt \$\{MY\_LOCAL\}/bin} is the path to where the User compiles the main programs into executables. 

{\tt trainingpoint\_optimizer} will read parameters from the {\tt smooth\_data/Options/tpo\_options.txt} file and from the {\tt smooth\_data/Info/prior\_info.txt} files. It will then write the information about the training points in the directory {\tt smooth\_data/FullModelRuns/}. Within the directory, a sub-directory will be created for each training point, named {\tt run0/, run1/, run2/}$\cdots$. Within each subdirectory, {\tt trainingpoint\_optimizer} creates a file {\tt runI/mod\_parameters.txt} for the I$^{\rm th}$ training point. For example, the {\tt run0/mod\_parameters.txt} file might be
{\tt\begin{verbatim}
   NuclearCompressibility     229.08
   ScreeningMass              0.453
   Viscosity                  0.192
\end{verbatim}
}
At this point, it is up to the User to run their full model at each training point and create a file {\tt runI/obs.txt}, which stores values of the observables at those training points as calculated by the full model. 

\subsection{Replacing {\it Training Point Optimizer} with Other Choices of Training Points}

If the User desires to use their own procedure for training points, or if the User wishes to augment the {\it Training Point Optimizer} points with additional training points, the User can write their own files {\tt runI/mod\_parameters.txt}. The emulator should work fine, though the User should remember that when training the emulator (see Sec. \ref{sec:emulator}) the emulator parameter describing with {\tt runI} directories to apply should be modified.

There is one warning to be issued when choosing training points for tuning {\it Smooth Emulator}. When solving for the Taylor coefficients, the emulator uses the factor of those coefficients (products of $\theta_i$) in the linear algebra routine. However, if those coefficients are not linearly independent, the solution becomes undetermined. For example, if two of the training points were exactly the same, it would fail. 

\end{document}
