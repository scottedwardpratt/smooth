\documentclass[UserManual.tex]{subfiles}
\begin{document}
\setcounter{section}{7}
\section{Template-Based Tutorial}\label{sec:tutorial}

\subsection{Overview}
A template project directory is provided that the User may copy to their own space, then use this as a foundation from which to embark on their own analysis. This directory includes information files, describing the parameter priors and the observables, that correspond to an artificial model that is also provided as a template. Working through the steps in this section constitutes a tutorial, both for running {\it Simplex Sampler} and for running {\it Smooth Emulator}.

This section describes the steps of how the User would
\begin{enumerate}\itemsep=0pt
\item Copy the required files from the template directory to the User's space, and compile the main programs.
\item Set up the information files describing the priors and observable names.
\item Run {\it Simplex Sampler} to generate the model-parameter values at which the full model will be trained.
\item Run a full model to generate the observables for each of the full-model runs.
\item Tune {\it Smooth Emulator} and write the coefficients to file.
\item Run a program that prompts the User for the coordinates of a point in parameter space, then returns the emulator prediction with its uncertainty.
\end{enumerate}

\subsection{Installation and Compilation}
After completing the necessary prerequisites listed in section \ref{sec:installation}[Installation] and following the steps outlined in section \ref{sec:installation}[Prerequisites] to install the required cmake, eigen, and gsl libraries, and setting the Home Environment Variable by creating the Home Directory as described in section \ref{sec:installation}[Making Home Directory and Setting Home Environment Variable], the user must proceed to clone the smooth and commonutils directories and compile the libraries, as explained in sections \ref{sec:installation}[Downloading] and [Compiling Libraries].

Then, the user can establish a personalized project directory by duplicating the project\_template directory onto their computer. The User should copy the directories\\ {\tt \$\{GITHOME\_BAND\_SMOOTH\}/templates/mylocal} and {\tt \$\{GITHOME\_BAND\_SMOOTH\}/templates/myproject} to a location in their personal space. We will refer to the User's two new directories as {\tt \$\{MY\_LOCAL\}/} and {\tt \$\{MY\_PROJECTS\}/}. For the purpose of this tutorial, the User must compile three main programs. This requires first changing into the {\tt \$\{MY\_LOCAL\}/main\_programs/} directory and entering:\\
{\tt
\begin{verbatim}
   ${MY\_LOCAL}/main_programs% cmake .
   ${MY\_LOCAL}/main_programs% make 
\end{verbatim}
}
Several executables should now appear in {\tt \$\{MY\_LOCAL\}/bin/}, {\tt simplex, smoothy\_tune, mcmc} and others. The User might find in convenient to add {\tt \$\{MY\_LOCAL\}/bin} to their path. The reason these are compiled in the User's space, separate from the main libraries, is that the User may well wish to create their own main programs, and this arrangement allows the User to compile their own versions, while leaving the original programs from the templates directory unchanged. 

For the purpose of the tutorial, there are also some fake full models included in the distribution. For the User's project the fake full model, which is very fast numerically, will be replaced by their own numerically intensive real full model. To compile the fake full model used in the tutorial the User should change into the {\tt \$\{MY\_LOCAL\}/main\_programs/} directory and enter:
{\tt
\begin{verbatim}
   ${MY_LOCAL}/fakemodels% cmake .
   ${MY_LOCAL}/fakemodels% make
\end{verbatim}
}
This particular fake full model has six model parameters and six observables, all with names in common use by the RHIC community. The output has absolutely no physical motivation, other than providing some arbitrary functions to emulate. The executable should appear in {\tt \$\{MY\_LOCAL\}/bin/}.

\subsection{Creating Necessary Info Files}
The User will run the software from the {\tt \$\{MY\_PROJECTS\}/} directory. Before a User can run {\it Simplex Sampler} they must create information files that describe the model-parameter priors and list the observable names. Both files are in the {\tt \$\{MY\_PROJECTS\}} directory. The first file is {\tt \$\{MY\_PROJECTS\}/Info/modelpar\_info.txt}. For the purposes of this tutorial, a file already exists,
{\tt
\begin{verbatim}
   compressibility         uniform  150   300
   etaovers                uniform  0.05  0.32
   initial_flow            uniform  0.3   1.2
   initial_screening       uniform  0.0   1.0
   quenching_length        uniform  0.5   2.0
   initial_epsilon         uniform  15.0  30.0
\end{verbatim}
}
This implies that the model has four parameters. The names, without much inspiration, are {\tt par1}, {\tt par2}, {\tt par3} and {\tt par4}. These names would normally be more descriptive, e.g. {\tt NuclearCompressibility}. The second entry in each line is either {\tt uniform} or {\tt gaussian}. If the parameter is {\tt uniform}, the last two numbers represent the range of the uniform prior, $x_{\rm min}$ and $x_{\rm max}$. If the second entry is {\tt gaussian} the third entry represents the center of the Gaussian distribution and the fourth represents the width. For a full model, the User would replace this model with one appropriate for their own model.

The second file is {\tt \$\{MY\_PROJECTS\}/Info/observable\_info.txt}. This describes output values from the model. In the template, the provided file is
{\tt
\begin{verbatim}
   meanpt_pion    100      
   meanpt_kaon    200      
   meanpt_proton  300      
   Rinv           1.0      
   v2             0.2     
   RAA            0.5
\end{verbatim}
}
The first entry in each line simply provides the names of the observable which will be processed in the Bayesian analysis.  The second entry is used by {\tt Smooth Emulator} during tuning, but only if a Monte Carlo method is used, and then is only used to seed the Monte Carlo search. If the analytical method is used for tuning (which is recommended) this parameter is irrelevant.

\subsection{Running {\it Simplex Sampler}}

Both {\it Simplex Sampler} and {\it Smooth Emulator} have options. These are provided in parameter files. For this tutorial, the provided parameter file is {\tt \$\{MY\_PROJECTS\}/parameters/simplex\_parameters.txt}. The provided file is
{\tt
\begin{verbatim}
   #Simplex_LogFileName    simplexlog.txt # comment out to direct output to screen
   Simplex_TrainType       2              # Must be 1 or 2             
   Simplex_ModelRunDirName modelruns      # Directory with training pt. info
\end{verbatim}
}
Because the first line is commented, the output of {\it Simplex Sampler} will be to the screen. Otherwise it would go to the specified file. By setting {\tt Simplex\_TrainType=1}, the sampler will choose $n+1$ training points, where $n=4$ is the number of model parameters. Each point corresponds to the vertices of an $n+1$ dimensional simplex.  Finally, the parameter {\tt Simplex\_ModelRunDirName} is set to ``{\tt modelruns}''. This informs {\tt Simplex Sampler} to write the coordinates of each training point and the corresponding observables in the directory {\tt \$\{MY\_PROJECTS\}/rhic/modelruns/}. 

Now the user can run {\tt Simplex Sampler}, which must be run from the project directory. The only output is the number of training points.
 {\tt
\begin{verbatim}
   ${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/simplex
   NTrainingPts=28
\end{verbatim}
}
If one had set {\tt Simplex\_TrainType}=1, only seven training points would have been created. The programs writes information about the training points in the {\tt modelruns/} directory. Changing into that directory, there should now be 28 sub-directories, corresponding to the 28 training points: {\tt modelruns/run0}, {\tt modelruns/run1}, {\tt modelruns/run2}, {\tt modelruns/}$\cdots$. Each directory has one text file describing the training points. For example, the {\tt modelruns/run0/mod\_parameters.txt} file might be 
{\tt
\begin{verbatim}
   compressibility 190.282
   etaovers 0.14892
   initial_flow 0.664958
   initial_screening 0.426807
   quenching_length 1.16036
   initial_epsilon 21.7424
\end{verbatim}
}
This describes the six model parameters, which will serve as the input for the first full model run.  The next step will be to run the full model for the parameters in each directory. Thus for {\tt Simplex\_Traintype=1}, one would need 7 full-model runs, and for {\tt Simplex\_Traintype=2}, one would need to do 28 full-model runs. The corresponding observables will be written in the files {\tt modelruns/runI/obs.txt}

\subsection{Running the Fake Full Model}
Once the training points have been generated, the user will run a full model based on the given structure, tailored to address their specific problem. For the tutorial, a fake full model is provided. It reads the model-parameter values in each {\tt modelruns/runI/mod\_parameters.txt} file and writes the corresponding observables in {\tt modelruns/runI/obs.txt}. The output should be as follows:
{\tt
\begin{verbatim}
   ${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/fakerhic
   NTraining Pts=28
   NPars=6
\end{verbatim}
}
The output simply verifies the number of model parameters and the number of training points created by simplex.

Inspecting the {\tt modelruns/run0/obs.txt} file,
{\tt
\begin{verbatim}
   meanpt_pion   418.821195  1.000000
   meanpt_kaon   715.592889  2.000000
   meanpt_proton 1079.482871 3.000000
   Rinv          5.004248    0.010000
   v2            0.178353    0.002000
   RAA           0.553416    0.005000
\end{verbatim}
}
The second entry of each line is the value of the specified observable for that specific training point. The last entry is the random uncertainty associated with the full model. This is only relevant if the model has random fluctuations, meaning the re-running the model at the same point might result in different output. For this tutorial, the emulator will not consider such fluctuations (there is an emulator parameter that can be set to either consider the randomness or ignore it), so the third entry on each line is superfluous.

\subsection{Running {\it Smooth Emulator}}
To tune the emulator, the User will run {\tt \$\{MY\_LOCAL\}/bin/SmoothEmulator\_tune} which should have been compiled in the directions above. The User needs to edit one additional file a this point, the parameter file that sets numerous options for {\it Smooth Emulator}. For the template used in this tutorial, that file is

{\tt
\begin{verbatim}
#SmoothEmulator_LogFileName smoothlog.txt # comment out for interactive running
  SmoothEmulator_LAMBDA 2.0 # smoothness parameter
  SmoothEmulator_MAXRANK 5
  SmoothEmulator_ConstrainA0 false
  SmoothEmulator_ModelRunDirName modelruns
  SmoothEmulator_TrainingPts 0-27
  SmoothEmulator_UsePCA   false
  SmoothEmulator_TuneExact true
 #
 # These are only used if you are using MCMC tuning rather than Exact method
  SmoothEmulator_TuneChooseMCMC false # set false if NPars<5
  SmoothEmulator_TuneChooseMCMCPerfect false #
  SmoothEmulator_MCMC_NASample 8  # No. of coefficient samples
  SmoothEmulator_MCStepSize 0.01
  SmoothEmulator_MCMC_CutoffA false # Used only if SigmaA constrained by SigmaA0
  SmoothEmulator_MCSigmaAStepSize 1.0  #
  SmoothEmulator_MCMCUseSigmaY false # If false, also varies SigmaA
  SmoothEmulator_MCMC_NMC 20000  # Steps between samples 
 #
 # This is for the MCMC search of parameter space (not for the emulator tuning)
 MCMC_METROPOLIS_STEPSIZE 0.01
\end{verbatim}
}
The parameters are described in detail in Sec. \ref{sec:emulator}. Because {\tt SmoothEmulator\_TuneExact} is set to {\tt true}, the Monte Carlo methods are not invoked and none of the parameters with {\tt MCMC} in their names are relevant. The most relevant parameter is setting the smoothness parameter. Also, it is important to make sure that {\tt SmoothEmulator\_TrainingPts} is set to the correct number of training points. The Constrain A0 parameter decides where the first term of the Taylor expansion is used to estimate the variance of the coefficients, which then affects the emulator's estimate of its uncertainty.

Now, running {\tt smoothy\_tune}, produces the following output,

{\tt
\begin{verbatim}
${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/smoothy_tune
 ---- Tuning for meanpt_pion ----
 ---- Tuning for meanpt_kaon ----
 ---- Tuning for meanpt_proton ----
 ---- Tuning for Rinv ----
 ---- Tuning for v2 ----
 ---- Tuning for RAA ----
\end{verbatim}
}
The program generates Taylor coefficients which are saved in the {\tt coefficients/} directory. Each observable has its own sub-directory with its name. In this case, {\tt smoothy\_tune} created the directories, {\tt coefficients/rhic/RAA}, {\tt coefficients/Rinv}, {\tt coefficients/menapt\_kaon}, {\tt coefficients/meanpt\_pion}, {\tt coefficients/meanpt\_proton} and {\tt coefficients/v2}. Within each of these sub-directories {\tt smoothy\_tune} created files {\tt meta.txt}, {\tt ABest.txt} and {\tt BetaBest.txt}.The number or parameters, the maximum rank of the Taylor expansion and the overall number of Taylor coefficients are give in {\tt meta.txt}. The file {\tt ABest.txt} provides the actual coefficients of the Taylor expansion, and {\tt BetaBest.txt} gives an array used to calculate the uncertainty. If one of the Monte Carlo methods is chosen, rather than the default analytic tuning method, the file {BetaBest.txt} is replaced by several files, {\tt sample0.txt, sample1.txt}$\cdots$, which provide several samples of Taylor coefficients. For the tutorial, the parameter file {\tt parameters/emulator\_parameters.txt} has the parameters set to use apply analytic tuning rather than Monte Carlo tuning.

\section{Testing the Emulator at the Training Points}
{\it Smooth Emulator} should return the training values at the training points. If one runs the executable {\tt smoothy\_train\_test}, it will first read in the coefficient information along with the training information. The program then emulates the model at the training points and compares the emulated value to the training value. Running the program gives the output:
{\tt
\begin{verbatim}
${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/smoothy_train_test
 --- TESTING AT TRAINING POINTS ----
 ------ itrain=0 --------
 Y[0]= 4.188e+02 =?  4.188e+02,    SigmaY_emulator= 1.78365e-07
 Y[1]= 7.156e+02 =?  7.156e+02,    SigmaY_emulator= 2.81059e-07
 Y[2]= 1.079e+03 =?  1.079e+03,    SigmaY_emulator= 4.08783e-07
 Y[3]= 5.004e+00 =?  5.004e+00,    SigmaY_emulator= 3.15227e-09
 Y[4]= 1.784e-01 =?  1.784e-01,    SigmaY_emulator= 1.08732e-10
 Y[5]= 5.534e-01 =?  5.534e-01,    SigmaY_emulator= 4.26570e-10
 ------ itrain=1 --------
 Y[0]= 4.744e+02 =?  4.744e+02,    SigmaY_emulator= 1.82174e-07
 Y[1]= 7.156e+02 =?  7.156e+02,    SigmaY_emulator= 2.87061e-07
 Y[2]= 1.066e+03 =?  1.066e+03,    SigmaY_emulator= 4.17513e-07
 Y[3]= 5.004e+00 =?  5.004e+00,    SigmaY_emulator= 3.21959e-09
 Y[4]= 1.784e-01 =?  1.784e-01,    SigmaY_emulator= 1.11054e-10
 Y[5]= 5.533e-01 =?  5.533e-01,    SigmaY_emulator= 4.35679e-10
 ------ itrain=2 --------
 Y[0]= 4.437e+02 =?  4.437e+02,    SigmaY_emulator= 3.01087e-07
 Y[1]= 7.846e+02 =?  7.846e+02,    SigmaY_emulator= 4.74437e-07
 Y[2]= 1.073e+03 =?  1.073e+03,    SigmaY_emulator= 6.90041e-07
 Y[3]= 5.004e+00 =?  5.004e+00,    SigmaY_emulator= 5.32114e-09
 Y[4]= 1.784e-01 =?  1.784e-01,    SigmaY_emulator= 1.83543e-10
 Y[5]= 6.175e-01 =?  6.175e-01,    SigmaY_emulator= 7.20065e-10
 ------ itrain=3 --------
 Y[0]= 4.457e+02 =?  4.457e+02,    SigmaY_emulator= 2.47694e-07
 Y[1]= 6.842e+02 =?  6.842e+02,    SigmaY_emulator= 3.90304e-07
 Y[2]= 1.182e+03 =?  1.182e+03,    SigmaY_emulator= 5.67674e-07
\end{verbatim}
$\vdots$
}
The observables, $Y[0]\cdots Y[27]$ should be identical and the uncertainties at the training points should be zero. The fact that the uncertainties are not exactly zero derives from the numerical accuracy of the linear algebra routines.

\section{Generating Emulated Observables at Given Points}
Finally, now that the emulator is tuned, one may wish to generate emulated values for the observables for specified points in model-parameter space. A sample program, {\tt \$\{MY\_LOCAL\}/bin/smoothy\_calcobs} is provided to illustrate how this can be accomplished. If one invokes the executable, using the same parameters as those used by {\tt smoothy\_tune}, the User is prompted to enter the coordinates of a point in model-parameter space, after which {\tt smoothy\_calcobs} prints out the observables. In this case, for the case where {\tt compressibility=205}, {\tt etaovers=0.2}, {\tt initial\_flow=0.7}, {\tt initial\_screening=0.4}, {\tt quenching\_length=1.2} and{\tt initial\_epsilon=23.0}

{\tt
\begin{verbatim}
${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/smoothy_calcobs
 Prior Info
 #   ParameterName Type   Xmin_or_Xbar  Xmax_or_SigmaX
  0: compressibility     uniform        150        300
  1: etaovers            uniform        0.05       0.32
  2: initial_flow        uniform        0.3        1.2
  3: initial_screening   uniform          0          1
  4: quenching_length    uniform        0.5          2
  5: initial_epsilon     uniform         15         30
 Enter value for compressibility:
 205
 Enter value for etaovers:
 .2 
 Enter value for initial_flow:
 .7
 Enter value for initial_screening:
 0.4
 Enter value for quenching_length:
 1.2
 Enter value for initial_epsilon:
 23.0
 ---- EMULATED OBSERVABLES ------
 meanpt_pion = 425.843 +/- 2.15299
 meanpt_kaon = 747.019 +/- 3.39257
 meanpt_proton = 1084.91 +/- 4.93428
 Rinv = 5.17076 +/- 0.03805
 v2 = 0.181905 +/- 0.00131247
 RAA = 0.59458 +/- 0.00514898
\end{verbatim}
}
Note that the uncertainties for the emulation are not effectively zero, as each set of the 8 sets of coefficients provides an an emulator that exactly reproduces the training points.

Of course, it is unlikely the User will wish to enter model parameters interactively as was done above. To incorporate {\tt Smooth Emulator} into other programs, the User should inspect the main programs, e.g. {\tt \$\{MY\_LOCAL\}/main\_programs/smoothy\_calcobs\_main.cc}. The User can then design their own program based on this source code, and compile and link it by editing\\ {\tt \$\{MY\_LOCAL\}/main\_programs/CMakeLists.txt}. By editing the CMake file, replacing the lines unique to {\tt smoothy\_calcobs}, one can easily compile new executables based on the User's main programs. To understand what might be involved, the source code in {\tt \$\{MY\_LOCAL\}/main\_programs/SmoothEmulator\_calcobs\_main.cc} is
{\tt
\begin{verbatim}
#include "msu_smoothutils/parametermap.h"
#include "msu_smooth/master.h"
#include "msu_smoothutils/log.h"
using namespace std;
int main(){
   NMSUUtils::CparameterMap *parmap=new CparameterMap();
   parmap->ReadParsFromFile("parameters/emulator_parameters.txt");
   NBandSmooth::CSmoothMaster master(parmap);
   master.ReadCoefficientsAllY();
   NBandSmooth::CModelParameters *modpars=new NBandSmooth::CModelParameters(); // contains info about single point
   modpars->priorinfo=master.priorinfo;
   master.priorinfo->PrintInfo();

   // Prompt user for model parameter values
   vector<double> X(modpars->NModelPars);
   for(unsigned int ipar=0;ipar<modpars->NModelPars;ipar++){
      cout << "Enter value for " << master.priorinfo->GetName(ipar) << ":\n";
      cin >> X[ipar];
   }
   modpars->SetX(X);
   
   //  Calc Observables
   NBandSmooth::CObservableInfo *obsinfo=master.observableinfo;
   vector<double> Y(obsinfo->NObservables);
   vector<double> SigmaY(obsinfo->NObservables);
   master.CalcAllY(modpars,Y,SigmaY);
   cout << "---- EMULATED OBSERVABLES ------\n";
   for(unsigned int iY=0;iY<obsinfo->NObservables;iY++){
      cout << obsinfo->GetName(iY) << " = " << Y[iY] << " +/- " << SigmaY[iY] << endl;
   }

   return 0;
}
\end{verbatim}
}
The above illustrates how one can write a code that 
\begin{itemize}\itemsep=0pt
\item[a)] Reads the parameter file
\item[b)] Creates a {\it master} emulator file (called master because it includes an emulator for each observable)
\item[c)] Creates a model-parameters object, {\tt modpars}, that stores the coordinates of the model-parameter point
\item[d)] Reads in the model parameters interactively
\item[e)] Calculates the observables from the emulator
\item[f)] Prints out the emulated observable and the uncertainty for for the emulator
\end{itemize}

\subsection{Exploring the Posterior via Markov-Chain Monte-Carlo}

Given the experimental information, which is stored in project directory in {\tt Info/experimental\_info.txt}, one can then use the tuned emulator to explore the posterior likelihood through MCMC, which works via a Metropolis algorithm. The file {\tt Info/experimental\_info.txt} provided in the template is:
{\tt
\begin{verbatim}
meanpt_pion     492     30   0.0
meanpt_kaon     734.2   40   0.0
meanpt_proton   1131.2  50   0.0
Rinv            5.495   0.2  0.0
v2              0.6375  0.1  0.0
RAA             0.4395  0.1  0.0
\end{verbatim}}
The first column is the list of observable names, which should be identical to those listed in {\tt Info/observable\_info.txt}. The second and third columns lists the experimental measurement, $Y_a$, and the experimental uncertainty, $\sigma_{a}^{\rm exp}$. The last column lists the additional uncertainty due to errors, $\sigma_a^{\rm theory}$, i.e. missing physics, in the theoretical model. For the purposes of comparing theory to data, only the combination $(\sigma_a^{\rm exp})^2+(\sigma_a^{\rm exp})^2$ comes into play, because this combination appears in the likelihood for the posterior,
\begin{align*}\eqnumber
\mathcal{L}(\vec{\theta})&=\prod_{a}\frac{1}{\sqrt{2\pi(\sigma_a^{\rm tot})^2}}
\exp\left\{-\frac{(Y_a(\vec\theta)-Y_{a}^{\rm exp})^{2}}{2(\sigma_a^{\rm tot})^2}\right\}\\
(\sigma_a^{\rm tot})^2&=(\sigma_a^{\rm exp})^2+(\sigma_a^{\rm theory})^2+(\sigma_a^{\rm emu})^2.
\end{align*}
Whereas the emulator uncertainty, $\sigma_a^{\rm emu}$, depends on the location in parameter space, $\vec{\theta}$, the other two contributions are assumed to be independent of $\vec{\theta}$.

There are special parameters for the MCMC. These are stored in {\tt Info/mcmc\_parameters.txt}. For the tutorial template, that file is
{\tt
\begin{verbatim}
# This is for the MCMC search of parameter space
 # (not for the emulator tuning)
 MCMC_LANGEVIN false
 MCMC_METROPOLIS_STEPSIZE 0.04
 MCMC_LANGEVIN_STEPSIZE 0.5
 MCMC_NBURN  10000
 MCMC_NTRACE 10000
 MCMC_NSKIP  5
 RANDY_SEED  12345
\end{verbatim}}
The first parameter, {\tt MCMC\_LANGEVIN} should be set to {\tt false}, as the Langevin MCMC (as opposed to the Metropolis version) is under development. The Metropolis stepsize should be adjusted so that the Metropolis success rate is approximately one half. The success rate prints out when the {\tt mcmc} code runs. If the success rate is anywhere between 20 and 80\%, this should be fine. But, if the rate is close to zero or 100\%, the efficiency of the procedure suffers. It is recommended to run the MCMC code with a modest number of steps, then adjust the stepsize accordingly.

The parameter {\tt MCMC\_NBURN} sets the number of Metropolis steps to be used in the ``burn-in'' stage, i.e. before one begins to store the trace. The number of elements to store in the trace in {\tt MCMC\_NTRACE}, and {\tt MCMC\_NSKIP} sets the number of steps to skip before storing a new point in the trace. Thus, if {\tt MCMC\_NTRACE} is one million and if {\tt MCMC\_NSKIP}=5, then the procedure will perform 5 million steps, and store every fifth one, leading to one million stored points in the trace. Finally, {\tt RANDY\_SEED} sets the random number seed. 

Running the MCMC program gives the following output:
{\tt
\begin{verbatim}
 ${MY_PROJECTS}/rhic% ${MY_LOCAL}/bin/mcmc
 At beginning of Trace, LL=-15.741889
 At end of trace, best LL=-4.920212
 Best Theta=
 0.152189  0.185884  0.111146  0.114218  0.205719  0.174787  
 Metropolis success percentage=65.970000
 finished burn in
 At beginning of Trace, LL=-6.781018
 finished 10%
 finished 20%
 finished 30%
 finished 40%
 finished 50%
 finished 60%
 finished 70%
 finished 80%
 finished 90%
 finished 100%
 At end of trace, best LL=-4.849059
\end{verbatim}}
Here {\tt best LL} refers to the log-likelihood and {\tt Best Theta} refers to the value of $\vec{\theta}$ that gave the maximum log-likelihood. Values of {\tt Best Theta} and {\tt best LL} are given after the burn-in and after the trace. 

The trace is written to {\tt mcmc\_trace/trace.txt}. It is in the format 
{\tt
\begin{verbatim}
theta_1	theta_2 theta_3 theta_4 ...
theta_1	theta_2 theta_3 theta_4 ...
theta_1	theta_2 theta_3 theta_4 ...
\end{verbatim}}
\vspace*{-18pt}\hspace*{82pt}$\vdots$
If {\tt MCMC\_NTRACE} is set to a million, there would be a million lines in the file. The program also calculated various covariances:
$\langle\langle\delta\theta_i\delta\theta_j\rangle\rangle$,  $\langle\langle\delta\theta_i\delta Y_a\rangle\rangle$, and $\langle\langle\delta Y_a\delta Y_b\rangle\rangle$. The quantities $\langle\langle....\rangle\rangle$ refer to averages over the  posterior, i.e. averages over the trace. The eigenvalues and eigenvectors of $\langle\langle\delta\theta_i\delta\theta_j\rangle\rangle$ are also recorded. Hopefully, the User will find the file names in {\tt mcmc\_trace/} to be self-explanatory.

As described in Sec. \ref{sec:theory} the covariances in the posterior can also be used to quantify the resolving power of specific observables to constrain specific parameters. One such measure is
\begin{align*}\eqnumber
\mathcal{R}_{ia}&\equiv\frac{d\langle\langle\theta_i\rangle\rangle}{dY_a^{\rm exp}}\langle \delta Y_a^2\rangle^{1/2}.
\end{align*}
This quantifies how the posterior value of $\theta_i$ changes as the experimental value changes if the experimental value, $Y_a^{\rm exp}$, changes an amount characteristic of the variance of $Y_a$ across the prior. Given that there may not be a set of training points calculated uniformly across the prior, the final factor is estimated as described in Sec. \ref{sec:theory}. Higher values of $\mathcal{R}_{ia}$ for different $a$ demonstrate the relative contributions of different observables $a$ to constrain a model parameter $i$. The resolving power matrix is written to {\tt mcmc\_trace/ResolvingPower.txt}.

Two python scripts (using MATPLOTLIB) are provided to provide graphical insight into the posterior likelihood and into the resolving power. To view the likelihood, the User can change into the directory {\tt figs/posterior}. Copy the file {\tt mcmc\_trace/trace.txt} to that directory. Also copy the file {\tt Info/modelpar\_info.txt} to that directory and then edit that file as described in {\tt figs/posterior/directions.txt}. The edits involve erasing some columns and adding a column with the model-parameter names in matplotlib (similar to LaTeX) format. For example, the template already has such a file:
{\tt
\begin{verbatim}
compressibility        uniform   $\kappa$
etaovers               uniform   $\eta/s$
initial_flow           uniform   Init.Flow
initial_screening      uniform   Screening
quenching_length       uniform   $\lambda_{\rm quench}$
initial_epsilon        uniform   $\epsilon_0$
\end{verbatim}}
The first two columns are the same as the first two columns in {\tt Info/modelpars\_info.txt}, while the third column lists the names of the model parameters used for the figure. Next, simply enter the command:
{\tt
\begin{verbatim}
${MY_PROJECT}/figs/posterior% python3 posterior.py
\end{verbatim}}
The script should create a file {\tt posterior.pdf}, which looks like:

\parbox{4.5in}{\centerline{\includegraphics[width=4.5in]{posterior_rhic.pdf}}}
~~\parbox{2.0in}{Projections for the posterior likelihood from the MCMC trace. The contour lines represent $1-\sigma$, $2-\sigma$ and $3-\sigma$ likelihoods.}

The likelihood is projected for individual model parameters, or for pairs. The plot is in terms of the scaled variables, $\theta_i$. To translate to the true model-parameter ranges, one can look at the {\tt Info/modelpars\_info.txt} file, which gives the prior ranges of the model parameters before they are scaled to the -1 to 1 range. The file {\tt figs/posterior/directions.txt} shows how the User can alter plot. For example there is a line in the python script, {\tt ParsToPlot=[1,2,3,4,5,0]}, which the User can edit to change the ordering of the model-parameters, and to choose which model parameters are considered.

Similarly, one can plot the resolving power. The User can visit the directory {\tt figs/resolvingpower/}. The User should copy the files {\tt mcmc\_trace/ResolvingPower.txt} and {\tt Info/modelpar\_info.txt} to this directory, editing the {\tt Info/modelpar\_info.txt} as was done for the posterior visualization figures described above. The User must also copy over the {\tt Info/observable\_info.txt} file and edit it in a similar fashion. In the template file is
{\tt
\begin{verbatim}
meanpt_pion          $\langle p_t\rangle_{\pi}$
meanpt_kaon          $\langle p_t\rangle_{K}$
meanpt_proton        $\langle p_t\rangle_{p}$
Rinv                 $R_{\rm inv}$
v2                   $v_2$
RAA                  $R_{AA}$
\end{verbatim}}
The first column should be exactly the same as the first line in the original file. The figure can now be produced via the command
{\tt
\begin{verbatim}
${MY_PROJECT}/figs/posterior% python3 RP.py
\end{verbatim}}
The figure should look like:

\parbox{4.0in}{\centerline{\includegraphics[width=4.0in]{RP_rhic.pdf}}}
\parbox{2.5in}{Resolving Power. Red bars represent positive correlations with $Y^{\rm exp}_a$ and $\theta_i$. Larger bars suggest that the particular observable contributes more to the constraint of the particular model parameter.}











\end{document}
