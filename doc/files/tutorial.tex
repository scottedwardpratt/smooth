\documentclass[UserManual.tex]{subfiles}
\begin{document}
\setcounter{section}{7}
\section{Tutorial}\label{sec:tutorial}

\subsection{Overview}

Working through the steps in this section constitutes a tutorial, consisting of the following steps.
\begin{enumerate}\itemsep=0pt
\item Copy the required files from the template directory to the User's space, and compile the main programs.
\item Set up the information files describing the priors and observable names.
\item Run {\it Simplex Sampler} to generate the model-parameter values at which the full model will be trained.
\item Run a full model to generate the observables for each of the full-model runs.
\item Tune {\it Smooth Emulator} and write the coefficients to file.
\item Run a program that prompts the User for the coordinates of a point in parameter space, then returns the emulator prediction with its uncertainty.
\end{enumerate}



\subsubsection{Setting up the Directory and Compiling the Main Programs}
The source code for the main programs
A analysis directory template is provided with the intention that the User will copy the directory to their own space, then use this as a foundation from which to embark on their own analysis. This template also provides the files for a tutorial. To copy the templates,
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
 % \textcolor{red}{cp -r ${SMOOTH_HOME}/AnalysisTemplate /Users/CarlosSmith/MyAnalysis}
}
\end{Verbatim}
}\vspace*{-12pt}
Here, {\tt ../MyAnalysis} can be replaced by any directory name the User chooses.

Installation and compilation is described in Sec. \ref{sec:installation}. As was defined in that section, the tutorial will refer to three locations with the short hand:

\vspace*{0.05in}

\begin{tabular}{rl}\hline
{\tt \$\{SMOOTH\_HOME\}} & \parbox{5in}{~\\Location of Git Repository, e.g.\\{\tt /Users/CarlosSmith/bandframework/software/SmoothEmulator}\\}\\
{\tt \$\{MY\_LOCAL\}} & \parbox{5in}{Can be placed anywhere. Executables are store in\\ {\tt \$\{MY\_LOCAL\}/bin} and main programs, and source codes for main\\ programs are found within {\tt \$\{MY\_LOCAL\}/software/main\_programs}\\}\\
{\tt \$\{MY\_ANALYSIS\}} & \parbox{5in}{Can be placed anywhere. Work spaces where parameter files, data, results and figures are created and stored. User may have several different such directories\\}\\
 \hline
\end{tabular}

The User should have also compile the main libraries, if not done already,

\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
   $\{SMOOTH_HOME\}/software% \textcolor{red}{cmake .}
   $\{SMOOTH_HOME\}/software% \textcolor{red}{make}
}
\end{Verbatim}
}\vspace*{-12pt}

and the main programs,
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
   $\{MY_LOCAL\}/software% \textcolor{red}{cmake .}
   $\{MY_LOCAL\}/software% \textcolor{red}{make}
}
\end{Verbatim}
}\vspace*{-12pt}

This will compile all the main source programs in {\tt \$\{MY\_LOCAL\}/software/MainPrograms/} and two programs used to generate files for the tutorial in {\tt \$\{MY\_LOCAL\}/software/TutorialPrograms/}. The repository was organized to encourage Users to edit any files in {\tt \$\{MY\_LOCAL\}/}. If the User wishes to restore any original files, a copy can be found at {\tt \$\{SMOOTH\_HOME\}/}.

Several executables should now appear in {\tt \$\{MY\_LOCAL\}/bin/}: {\tt trainingpoint\_optimizer, smoothy\_testattrainingpts,smoothy\_testvsfullmodel,smoothy\_calcobs,smoothy\_mcm}, which all involve the emulator. Tow other executables, {\tt fakefullmodel} and {\tt fakeinfo} are only used for the tutorial. The User might find it convenient to add {\tt \$\{MY\_LOCAL\}/bin} to their path. The reason these are compiled in the User's space, separate from the main libraries, is that the User may well wish to create their own main programs, and this arrangement allows the User to compile their own versions, while leaving the bulk of the source code unchanged. 

\subsection{Creating the Necessary Info Files}
The User will run the software from the {\tt \$\{MY\_ANALYSIS\}/} directory. Before a User can run the {\it Smooth Emulator} executables they must create text files that describe the model-parameter priors and list the observable names. These three files need to be in the {\tt \$\{MY\_ANALYSIS\}/smooth\_data/Info/} directory. The first file, {\tt smooth\_data/Info/prior\_info.txt}, describes the model parameters and their priors. This file is For the purposes of this tutorial, we consider a model with six model parameters:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
# par_name dist_type  xmin/centroid xmax/width  SensitivityScale
   par0 gaussian 0 100    1.00000
   par1 uniform -100 100  1.00000
   par2 gaussian 0 100    1.00000
   par3 uniform -100 100  1.00000
   par4 gaussian 0 100    1.00000
   par5 uniform -100 100  1.00000
}
\end{Verbatim}
}\vspace*{-12pt}
Thus, the model has six parameters. The second entry in each line is either {\tt uniform} or {\tt gaussian}. If the entry is {\tt uniform}, the last two numbers represent the range of the uniform prior, $x_{\rm min}$ and $x_{\rm max}$. If the second entry is {\tt gaussian} the third entry represents the center of the Gaussian distribution and the fourth represents the width. The final column represents the sensitivity scale, which must be between zero and 1.0. For the most impactful parameters, this should be set to 1.0. If a parameter is expected to provide little variance compared to the most impactful parameters, this should be set to some fraction. Roughly, if a parameter is expected to provide half as much variance as the most important parameters, it should be set to 0.5. For a full model, the User would replace this file with one appropriate for their own model. This file is required for training-point optimization, for tuning, and for the MCMC programs.  

The second file is {\tt smooth\_data/Info/observable\_info.txt}. This describes output values from the model. In the template, the file describes, in this case, ten observables:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
# ObservableName   ALPHA
   obs0 0
   obs1 0
   obs2 0
   obs3 0
   obs4 0
   obs5 0
   obs6 0
   obs7 0
   obs8 0
   obs9 0
}
\end{Verbatim}
}\vspace*{-12pt}
The first entry in each line simply provides the names of the observable which will be processed in the Bayesian analysis.  The second entry describes the point-to-point noise is used by both the training-point optimizing programs and by the emulator tuning programs. Here, {\tt ALPHA} is the noise as a fraction of the characteristic variation of the observable, $\sigma_Y$. Noise refers to variations of the model that would occur if the model were rerun with the same model parameters, which is typically due to some finite sampling inherent to the model, e.g. if the particle is calculating the average energy of particles using a simulation, and if a finite number of simulations and particles are sampled, the observable might vary from run to another. If one expects that random uncertainty to be 5\% of the variation of the observable throughout the model parameter space, then {\tt ALPHA} would be 0.05. {\it Smooth Emulator} software assumes that this noise is the same for all the full calculations of the same observable. I.e., if 20 training runs were performed at different points in model-parameter space, the software assumes that the {\tt ALPHA} was not much different at one training point than at another. If {\tt ALPHA} is set to zero, the emulator will exactly reproduce the observables from the full model when evaluated at the training points. If one performed training runs with simulations using grossly different numbers of events at one training point vs another, then this shortcoming could potentially be an issue. This file is required by both the tuning and MCMC programs.

The third, and final file, {\tt smooth\_data/Info/experimental\_info.txt}, describes the actual measurements and only comes into play at the time the MCMC is being performed. The file in the tutorial might look like this:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
obs0  57.840878  10.0 0.0
obs1  -85.613188  10.0 0.0
obs2  43.335184  10.0 0.0
obs3  40.159450  10.0 0.0
obs4  38.013337  10.0 0.0
obs5  64.492673  10.0 0.0
obs6  -195.300157  10.0 0.0
obs7  99.587427  10.0 0.0
obs8  7.963056  10.0 0.0
obs9  68.844147  10.0 0.0 
}
\end{Verbatim}
}\vspace*{-12pt}
The names of the observables must match those listed in the {\tt observable\_info.txt} file. The second column is the value reported by the experiment and the third column is the reported uncertainty. The uncertainty cannot be set to zero. The last column represents the model's uncertainty due to missing physics, i.e. not the uncertainty due to noise in the calculation. The contribution from random noise is incorporated into the emulator uncertainty. The uncertainty due to missing physics can be thought of as the error, or deviation from a perfect measurement, one might expect if one used the precisely correct parameters. One can think of this as the systematic error of the theory on which the model is built. For the purpose of the MCMC, the three uncertainties, that of the emulator, that from the experiment, and the model's systematic theoretical error, are all added in quadrature to provide the uncertainty relevant for calculating the log-likelihood in the MCMC.

The User needs to edit the three files above according to their project. For the purpose of this tutorial, the user should run the program {\tt fakeinfo}, which prompts the User for the number of model parameters and observables. 
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
   $\{MY_ANALYSIS\}/software% \textcolor{red}{$\{MY_LOCAL\}/bin/fakeinfo}
}
\end{Verbatim}
}\vspace*{-12pt}
This program creates the {\tt Info} files for a model with 6 model parameters and 10 observables. The three info files should appear like those shown above.

\subsection{Running {\tt trainingpoint\_optimizer}}

Before running {\tt trainingpoint\_optimizer} the User should inspect or edit the options file, {\tt smooth\_data/Options/tpo\_options.txt}. A template for this file is provided:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
TPO_Method  MCQuadratic  # can be "MC", "MCSphere", "MCSimplex", "MCSimplexPlus1", "MCQuadratic", "MCSphereQuadratic"
TPO_NTrainingPts    0   # only relevant for OptimizeMethod="MCSphere" or "MC"
TPO_NMC  10000
TPO_ALPHA 0.01 # does not have to be the same value used to train emulator
TPO_Include_LAMBDA_Uncertainty true # dangerous to set this to false
TPO_FullModelRunsDirName  smooth_data/FullModelRuns
#TPO_ReadPoints 0,1,4-7,10,11-13,21
#TPO_FreezePoints 0,1,4-7,10,11-13,21
}
\end{Verbatim}
}\vspace*{-12pt}
The options are described in detail in \



Now the user can run {\tt trainingpoint\_optimizer}, which is meant to be run interactively. This will generate a list of coordinates in model-parameter space for each of $N_{\rm train}$ training points. Simply run the program by calling the command. The output to the screen is:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
${MY_ANALYSIS}% \textcolor{red}{${MY_LOCAL}/bin/trainingpoint_optimizer}
NTrainingpts=28, NMC=10000, LAMBDA=2.500000, ALPHA=0.010000
Optimize_MC: at beginning: bestSigma2=0.014520
+++ finished 1%, expected accuracy=0.01331, success %=39, step size=0.0094491
+++ finished 2%, expected accuracy=0.011875, success %=33, step size=0.015213
+++ finished 3%, expected accuracy=0.01119, success %=20, step size=0.020842
 .
 .
+++ finished 99%, expected accuracy=0.0072313, success %=14, step size=0.00043523
+++ finished 100%, expected accuracy=0.0072312, success %=26, step size=0.00026549
}
\end{Verbatim}
}\vspace*{-12pt}
?????? The output ----------------------

The program writes information about the training points in the {\tt smooth\_data/FullModelRuns/} directory. Changing into that directory, there should now be 28 sub-directories, corresponding to the 28 training points: {\tt FullModelRuns/run0}, {\tt FullModelRuns/run1}, {\tt FullModelRuns/run2} $\cdots$. Each directory has one text file describing the training points. For example, the {\tt modelruns/run21/model\_parameters.txt} file might be 
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
 par0 -145.307
 par1 -6.8244
 par2 -67.4252
 par3 62.3719
 par4 -85.451
 par5 -29.6548
}
\end{Verbatim}
}\vspace*{-12pt}
This describes the six model parameters, which will serve as the input for that model run.  The next step will be to run the full model for the parameters in each directory.

In this example, {\tt trainingpoint\_optimizer} was run with {\tt TPO\_Method} set to {\tt MCQuadratic}. This choice automatically set the number of training points to 28 given that there were six model parameters and 28 points are required to fully constrain a quadratic fit. If the option had been set to {\tt MCSimplex}, 7 points would have been chosen, the minimum number to constrain a linear fit (If all the priors were Gaussian, this would result in a simplex arrangement). If {\tt TPO\_Method} had been set to {\tt MC}, {\tt trainingpoint\_optimizer} would have used the option {\tt TPO\_NTrainingPts} to set the number of training points. The full range of options is described in Sec. \ref{sec:tpo}.

\subsection{Running the Full Model}
Once the training points have been generated, the User will run the full model at each training point. For the tutorial, a fake full model is provided. It reads the model-parameter values in each {\tt smooth\_data/modelruns/runI/model\_parameters.txt} file and writes the corresponding observables for each point in model-parameter space, {\tt I}, in the file {\tt smooth\_data/modelruns/runI/obs.txt}.

The User needs to run the program {\tt fakefullmodel}, which is interactive. Running the model, the User should see something like:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
   ${MY_ANALYSIS}% \textcolor{red}{$\{MY_LOCAL\}/bin/fakefullmodel}
   NPars=6, NObs=10
   Creating Fake Model Data for 28 Training Pts
}
\end{Verbatim}
}\vspace*{-12pt}
The output first shows the number of model parameters, observables, and training points. The number of model parameters was found by reading {\tt smooth\_data/Info/prior\_info.tt} and the number of training points was determined by counting the number of files in {\tt smooth\_data/FullModelRuns}. The fake model is built on a Taylor expansion with random coefficients, chosen to be consistent with the form assumed by the emulator. This form is built on the choices of $\Lambda=3.0$ and $\Sigma_A=100$. 

Inspecting one of the output files, {\tt smooth\_data/FullMode/run0/obs.txt},
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
 obs0 77.444888
 obs1 -78.684124
 obs2 62.298517
 obs3 109.230134
 obs4 70.629874
 obs5 106.994538
 obs6 -196.814283
 obs7 9.158107
 obs8 -29.184781
 obs9 41.295872
}
\end{Verbatim}
}\vspace*{-12pt}
The second column provides the value of the specified observable for the full model at that specific training point. Additionally, {\tt fakefullmodel} created a directory {\tt smooth\_data/FullModelTestingRuns/} which stores information about full-model runs at randomly chosen points in the model-parameter space. These points and their corresponding data are not used for tuning. This data can be used later to test the emulator. The User need not create such files for their project, unless they wish to have some separate runs just for testing. 

\subsection{Running the Emulator}
Before building and tuning the emulator, the User needs to edit one additional file, the parameter file that sets numerous options for {\it Smooth Emulator}. This file is {\tt smooth\_data/Options/emulator\_options.txt}. For the template used in this tutorial, the contents of that file appear as
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
  # Location of output, comment out for interactive running
  # LogFileName smoothlog.txt
  SmoothEmulator_FixLambda false // If false will adjust optimize LAMBDA (false is default)
  # Smoothness parameter to start maximizing procedure
  SmoothEmulator_LAMBDA 2.5    # If SmoothEmulator_FixLambda is true, this fixes LAMBDA
                               #otherwise this is just a starting point for optimization search
  #
  # Choose read/write format for training data (SMOOTH or SURMISE), default is SMOOTH
  # If format is SMOOTH
  SmoothEmulator_TrainingFormat SMOOTH
  #SmoothEmulator_TrainingFormat SURMISE
  # Location of training data (if SMOOTH FORMAT) default is smooth_data/FullModelRuns
  SmoothEmulator_FullModelRunsDirName smooth_data/FullModelRuns
  # Location of training data (if SURMISE FORMAT) default is SurmiseTrainingPoints.txt
  #SmoothEmulator_SurmiseTrainingPointsFilename SurmiseTrainingPoints.txt
  # Location of Observables (if SURMISE FORMAT)
  #SmoothEmulator_SurmiseTrainingObsFilename SurmiseTrainingObs.txt
  #
  # Choose which runs to use for training. Can be set to "all" or as list, e.g. 1-5,8-12,15,18
  SmoothEmulator_TrainingPts all
  #
  # Below only used for testing agains full model runs not used for training.
  # Choose read/write format for testing data (SMOOTH or SURMISE), default is SMOOTH
  SmoothEmulator_TestingFormat SMOOTH
  #SmoothEmulator_TestingFormat SURMISE # Note this is not tested
  # For Smooth format default location of testing data is smooth_data/FullModelTestingRuns/
  SmoothEmulator_FullModelTestingRunsDirName smooth_data/FullModelTestingRuns
  #
  # List of points to be used for testing, all is default, can list, e.g. 1-5,8-12,15,18
  SmoothEmulator_TestingPts all
  #
  # When calculating uncertainty, include(or not) the contribution from Lambda varying
  SmoothEmulator_INCLUDE_LAMBDA_UNCERTAINTY true
}
\end{Verbatim}
}\vspace*{-12pt}
More options are described in detail in Sec. \ref{sec:emulator}. The tutorial will use the default format (not that of the SURMISE format) because the tutorial recorded the training-point information in the {\it Smooth Emulator} format.

There are two pre-constructed main programs to demonstrate the tutorial: {\tt \$\{MY\_LOCAL\}/bin/smoothy\_testattrainingpts} and {\tt \$\{MY\_LOCAL\}/bin/smoothy\_testvsfullmodel}. The corresponding source code for the first is {\tt \$\{MY\_LOCAL\}/software/MainPrograms/smoothy\_testattrainingpts\_main.cc},
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
#include "msu_smoothutils/parametermap.h"
#include "msu_smooth/master.h"
#include "msu_smoothutils/log.h"

using namespace std;
int main()\{
        NBandSmooth::CSmoothMaster master;
        master.TuneAllY();
        master.TestAtTrainingPts();
        return 0;
\}
}
\end{Verbatim}
}\vspace*{-12pt}
and the source code for the second is similarly simple,
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
#include "msu_smoothutils/parametermap.h"
#include "msu_smooth/master.h"
#include "msu_smoothutils/log.h"

using namespace std;
int main()\{
        NBandSmooth::CSmoothMaster master;
        master.TuneAllY();
        master.TestVsFullModel();
        return 0;
\}
}
\end{Verbatim}
}\vspace*{-12pt}
As one would guess from the source code, the first program builds and tunes the emulator for all the observables through the call {\tt master.TuneAllY()}. The second line, {\tt master.TestAtTrainingPts()}, runs the emulator at each of the training points and compares to the training values. These are interactive programs and running {\tt smoothy\_testattrainingpts} should give an output like:
vIn the tuning of the emulator, the hyper-parameters $\Lambda$ and $\Sigma_A$ are chosen. The program {\tt fakefullmodel} was built based on a randomized Taylor expansion consistent with $\Lambda=2.5$ set to 2.5 and $\Lambda=100$. Due to the finite number of training points (in this case 28) the emulator's value differs from these values. The output would continue to compare the emulator to the training points at each point, and for each of the 10 observables. The last column of the output is the emulator's uncertainty. 

The second program, {\tt smoothy\_testvsfullmodel} also constructs and tunes the emulator. But in this case, the program compares the full model to the emulated values away from the training points. The data for the full model was created by the {\tt fakefullmodel} program, and is found in {smooth\_data/FullModelTestingRuns}. Testing data was created for 50 random points in model-parameter space. Again, the source code is rather simple.
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
#include "msu_smoothutils/parametermap.h"
#include "msu_smooth/master.h"
#include "msu_smoothutils/log.h"

using namespace std;
int main()\{
        NBandSmooth::CSmoothMaster master;
        master.TuneAllY();
        master.TestVsFullModel();
        return 0;
\}
}
\end{Verbatim}
}\vspace*{-12pt}
To run the program,
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
$\{MY_ANALYSIS\}% smoothy_testvsfullmodel  
iY=0: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.857423
percent < 1 sigma = 44.000000
iY=1: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=0.965030
percent < 1 sigma = 72.000000
iY=2: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.138670
percent < 1 sigma = 65.000000
iY=3: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=3.380905
percent < 1 sigma = 65.000000
iY=4: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.080057
percent < 1 sigma = 58.000000
iY=5: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=0.833119
percent < 1 sigma = 77.000000
iY=6: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.698096
percent < 1 sigma = 54.000000
iY=7: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.739706
percent < 1 sigma = 46.000000
iY=8: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=1.089194
percent < 1 sigma = 63.000000
iY=9: (<(Y-Yreal)^2>/SigmaY^2_emulator)^1/2=0.874994
percent < 1 sigma = 80.000000
}
\end{Verbatim}
}\vspace*{-12pt}



One can now peruse the files in {\tt smooth\_data/output\_stuff/fullmodel\_testdata/} and see the comparison of the 50 emulator values to the values calculated by {\tt fakefullmodel}. 



Given that the tuning is very fast, there is little need to write the coefficients as any subsequent use of the emulators can simply repeat the tuning, rather than reading in the coefficients. However, for large numbers of training points and model parameters, it might make sense to used stored values of the emulator's hyper-parameters to avoid recalculating them. Those parameters are stored in {\tt smooth\_data/output\_stuff/sigmalambda.txt}.

Hopefully, the User will find the source codes used in this tutorial to be fairly self-explanatory. Nonetheless, detailed explanations can be found in Sec. \ref{sec:emulator}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exploring the Posterior via Markov-Chain Monte-Carlo}

Given the experimental information, which is stored in analysis directory in {\tt smooth\_data/Info/experimental\_info.txt}, one can then use the tuned emulator to explore the posterior likelihood through MCMC, which works via a Metropolis algorithm. The file {\tt smooth\_data/Info/experimental\_info.txt} provided in the template is:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
meanpt_pion     481.179     20    0.0
meanpt_kaon     757.872     30    0.0
meanpt_proton   1113.3      40    0.0
Rinv            6.27842     0.3   0.0
v2              0.382973    0.1   0.0
RAA             0.558367    0.1   0.0
}
\end{Verbatim}
}\vspace*{-12pt}
The first column is the list of observable names, which should be identical to those listed in {\tt smooth\_data/Info/observable\_info.txt}. The second and third columns lists the experimental measurement, $Y_a$, and the experimental uncertainty, $\sigma_{a}^{\rm exp}$. The last column lists the additional uncertainty due to errors, $\sigma_a^{\rm theory}$, i.e. missing physics, in the theoretical model. For the purposes of comparing theory to data, only the combination $(\sigma_a^{\rm exp})^2+(\sigma_a^{\rm exp})^2$ comes into play, because this combination appears in the likelihood for the posterior,
\begin{align*}\eqnumber
\mathcal{L}(\vec{\theta})&=\prod_{a}\frac{1}{\sqrt{2\pi(\sigma_a^{\rm tot})^2}}
\exp\left\{-\frac{(Y_a(\vec\theta)-Y_{a}^{\rm exp})^{2}}{2(\sigma_a^{\rm tot})^2}\right\}\\
(\sigma_a^{\rm tot})^2&=(\sigma_a^{\rm exp})^2+(\sigma_a^{\rm theory})^2+(\sigma_a^{\rm emu})^2.
\end{align*}
Whereas the emulator uncertainty, $\sigma_a^{\rm emu}$, depends on the location in parameter space, $\vec{\theta}$, the other two contributions are assumed to be independent of $\vec{\theta}$.

There are special parameters for the MCMC. These are stored in {\tt smooth\_data/Info/mcmc\_parameters.txt}. For the tutorial template, that file is
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
 # This is for the MCMC search of parameter space
 # (not for the emulator tuning)
 MCMC_LANGEVIN false
 MCMC_METROPOLIS_STEPSIZE 0.05
 MCMC_LANGEVIN_STEPSIZE 0.5
 MCMC_NBURN  100000
 MCMC_NTRACE 100000
 MCMC_NSKIP  5
 RANDY_SEED  12345
}
\end{Verbatim}
}\vspace*{-12pt}
The first parameter, {\tt MCMC\_LANGEVIN} should be set to {\tt false}, as the Langevin MCMC (as opposed to the Metropolis version) is under development. The Metropolis stepsize should be adjusted so that the Metropolis success rate is approximately one half. The success rate prints out when the {\tt mcmc} code runs. If the success rate is anywhere between 20 and 80\%, this should be fine. But, if the rate is close to zero or 100\%, the efficiency of the procedure suffers. It is recommended to run the MCMC code with a modest number of steps, then adjust the stepsize accordingly.

The parameter {\tt MCMC\_NBURN} sets the number of Metropolis steps to be used in the ``burn-in'' stage, i.e. before one begins to store the trace. The number of elements to store in the trace in {\tt MCMC\_NTRACE}, and {\tt MCMC\_NSKIP} sets the number of steps to skip before storing a new point in the trace. Thus, if {\tt MCMC\_NTRACE} is one million and if {\tt MCMC\_NSKIP}=5, then the procedure will perform 5 million steps, and store every fifth one, leading to one million stored points in the trace. Finally, {\tt RANDY\_SEED} sets the random number seed. 

\newpage

Running the MCMC program gives the following output:

\vspace*{-12pt}
{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
$\{MY_ANALYSIS\}/rhic% \textcolor{red}{\$\{MY_LOCAL\}/bin/mcmc}
At beginning of Trace, LL=-68.728122
At end of trace, best LL=1.223190
Best Theta=
0.233733  0.158783  0.201324  0.209576  0.008904  0.202032  
Metropolis success percentage=55.010000
finished burn in
At beginning of Trace, LL=-3.055348
finished 10%
finished 20%
finished 30%
finished 40%
finished 50%
finished 60%
finished 70%
finished 80%
finished 90%
finished 100%
At end of trace, best LL=1.375865
Best Theta=
0.236218  0.138233  0.183291  0.217895  0.033619  0.194723  
Metropolis success percentage=57.355500
writing Theta values, ntrace = 100001
writing X values, ntrace = 100001
}
\end{Verbatim}
}\vspace*{-12pt}
Here {\tt best LL} refers to the log-likelihood and {\tt Best Theta} refers to the value of $\vec{\theta}$ that gave the maximum log-likelihood. Values of {\tt Best Theta} and {\tt best LL} are given after the burn-in and after the trace. One can read the source file, {\tt \$\{MY\_LOCAL\}/software/main\_programs/mcmc\_main.cc}

The trace is written to {\tt mcmc\_trace/trace.txt}. It is in the format 
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
theta_1 theta_2 theta_3 theta_4 ...
theta_1 theta_2 theta_3 theta_4 ...
theta_1 theta_2 theta_3 theta_4 ...
}
\end{Verbatim}
}\vspace*{-12pt}
\vspace*{-18pt}\hspace*{82pt}$\vdots$

If {\tt MCMC\_NTRACE} is set to a million, there would be a million lines in the file. The program also calculated various covariances:
$\langle\langle\delta\theta_i\delta\theta_j\rangle\rangle$,  $\langle\langle\delta\theta_i\delta Y_a\rangle\rangle$, and $\langle\langle\delta Y_a\delta Y_b\rangle\rangle$. The quantities $\langle\langle....\rangle\rangle$ refer to averages over the  posterior, i.e. averages over the trace. The eigenvalues and eigenvectors of $\langle\langle\delta\theta_i\delta\theta_j\rangle\rangle$ are also recorded. Hopefully, the User will find the file names in {\tt mcmc\_trace/} to be self-explanatory.

As described in Sec. \ref{sec:theory} the covariances in the posterior can also be used to quantify the resolving power of specific observables to constrain specific parameters. One such measure is
\begin{align*}\eqnumber
\mathcal{R}_{ia}&\equiv\frac{d\langle\langle\theta_i\rangle\rangle}{dY_a^{\rm exp}}\langle \delta Y_a^2\rangle^{1/2}.
\end{align*}
This quantifies how the posterior value of $\theta_i$ changes as the experimental value changes if the experimental value, $Y_a^{\rm exp}$, changes an amount characteristic of the variance of $Y_a$ across the prior. Given that there may not be a set of training points calculated uniformly across the prior, the final factor is estimated as described in Sec. \ref{sec:theory}. Higher values of $\mathcal{R}_{ia}$ for different $a$ demonstrate the relative contributions of different observables $a$ to constrain a model parameter $i$. The resolving power matrix is written to {\tt mcmc\_trace/ResolvingPower.txt}.

\subsection{Making Plots}
Three python scripts (using MATPLOTLIB) are provided to provide graphical insight into the posterior likelihood, into the resolving power and for viewing how the emulator uncertainty compares to the discrepancies between full-model runs (not used for tuning) and emulated value. One can visit the {\tt \$\{MY\_ANALYSIS\}/figs/} directory and peruse the file {\tt directions.txt} for more detailed instructions of how to create the plots below. First, one must create two data files. These provide the names of observables and model parameters to be used by the plots.

The two files are given the same names as two files in {\tt smooth\_data/Info/}. The first is {\tt \$\{MY\_ANALYSIS\}/figs/modelpars\_info.txt}. It differs from the one in the {\tt smooth\_data/Info/} directory in that it has only three columns, though the first column is identical. The provided file is\\
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
compressibility           uniform    \$\\kappa\$
etaovers                  uniform    \$\\eta/s\$
initial_flow              uniform    Init.Flow
initial_screening         uniform    Screening
quenching_length          uniform    \$\\lambda_\{\rm quench\}\$
initial_epsilon           uniform    \$\\epsilon_0\$
}
\end{Verbatim}
}\vspace*{-12pt}
As one can see, the last column is used by MATPLOTLIB to label axes. The middle column is not currently used, but that might come into play if one wanted to alter the python scripts to use a different scale for gaussian priors.

The second required file is {\tt \$\{MY\_ANALYSIS\}/figs/observable\_info.txt}, and the provided version is\\
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
meanpt_pion          \$\\langle p_t\\rangle_\{\\pi\}\$
meanpt_kaon          \$\\langle p_t\\rangle_\{K\}\$
meanpt_proton        \$\\langle p_t\\rangle_{p}\$
Rinv                 \$R_\{\rm inv\}\$
v2                   \$v_2\$
RAA                  \$R_\{AA\}\$
}
\end{Verbatim}
}\vspace*{-12pt}
Again, the first column is identical to that in {\tt smooth\_data/Info/} and the second is used for labeling.

To graph the posterior likelihood, first be sure to run the {\tt mcmc} program, then change into the {\tt figs/posterior} directory and enter the command:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
${MY_ANALYSIS}/figs/posterior% \textcolor{red}{ln -s ../../smooth_data/mcmc_trace/trace.txt .}
${MY_ANALYSIS}/figs/posterior% \textcolor{red}{python3 posterior.py}
}
\end{Verbatim}
}\vspace*{-12pt}
One can replace ``{\tt ln -s}'' with ``cp'' or ``mv''. The script should create a file {\tt posterior.pdf}, which looks like:

\parbox{4.5in}{\centerline{\includegraphics[width=4.5in]{figs/posterior_rhic.pdf}}}
~~\parbox{2.0in}{Projections for the posterior likelihood from the MCMC trace. The contour lines represent $1-\sigma$, $2-\sigma$ and $3-\sigma$ likelihoods.}

The likelihood is projected for individual model parameters, or for pairs. The plot is in terms of the scaled variables, $\theta_i$. To translate to the true model-parameter ranges, one can look at the {\tt smooth\_data/Info/modelpars\_info.txt} file, which gives the prior ranges of the model parameters before they are scaled to the -1 to 1 range. The file {\tt figs/directions.txt} shows how the User can alter plot. For example there is a line in the python script, {\tt ParsToPlot=[1,2,3,4,5,0]}, which the User can edit to change the ordering of the model-parameters, and to choose which model parameters are considered.

Similarly, one can plot the resolving power. The User can visit the directory {\tt figs/resolvingpower/}. The User should copy the files {\tt mcmc\_trace/ResolvingPower.txt} and {\tt smooth\_data/Info/prior\_info.txt} to this directory, editing the {\tt smooth\_data/Info/prior\_info.txt} as was done for the posterior visualization figures described above. The User must also copy over the {\tt smooth\_data/Info/observable\_info.txt} file and edit it in a similar fashion. In the template file is
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
meanpt_pion          \$\\langle p_t\\rangle_\{\\pi\}\$
meanpt_kaon          \$\\langle p_t\\rangle_\{K\}\$
meanpt_proton        \$\\langle p_t\\rangle_\{p\}\$
Rinv                 \$R_\{\rm inv\}\$
v2                   \$v_2\$
RAA                  \$R_\{AA\}\$
}
\end{Verbatim}
}\vspace*{-12pt}
The first column should be exactly the same as the first line in the original file. After running the {\tt mcmc} program, the figure can be produced via the command
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
${MY_ANALYSIS}/figs/resolvingpower% \textcolor{red}{ln -s ../../smooth_data/mcmc_trace/ResolvingPower.txt .}
${MY_ANALYSIS}/figs/resolving power% \textcolor{red}{python3 RP.py}
}
\end{Verbatim}
}\vspace*{-12pt}
The figure should look like:

\parbox{4.0in}{\centerline{\includegraphics[width=4.0in]{figs/RP_rhic.pdf}}}
\parbox{2.5in}{Resolving Power. Red bars represent positive correlations with $Y^{\rm exp}_a$ and $\theta_i$. Larger bars suggest that the particular observable contributes more to the constraint of the particular model parameter.}

The third provided python script is in {\tt figs/YvsY/} and it compares full-model runs (not used for tuning) to the emulator. This is useful for seeing whether the emulator's error estimates are reasonable. First, one must run the program that writes out the emulator predictions for the full-model runs. This is accomplished by the command:
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
${MY_ANALYSIS}% \textcolor{red}{smoothy_testvsfullmodelalt}
meanpt_pion: 27 out of 50 points within 1 sigma
meanpt_kaon: 41 out of 50 points within 1 sigma
meanpt_proton: 26 out of 50 points within 1 sigma
Rinv: 32 out of 50 points within 1 sigma
v2: 39 out of 50 points within 1 sigma
RAA: 29 out of 50 points within 1 sigma
}
\end{Verbatim}
}\vspace*{-12pt}
If the uncertainty were perfectly stated, 68\% of the points would be within one standard deviation. In this case the fraction was higher, which suggests that the uncertainty was somewhat overstated.

To make the plot, first change into the {\tt figs/YvsY/} directory, and enter
\vspace*{-12pt}{\tt \begin{Verbatim}[commandchars=\\\{\}]
\codebox{
${MY_ANALYSIS}/figs/YvsY% \textcolor{red}{ln -s ../../smooth_data/fullmodel_testdata .}
${MY_ANALYSIS}/figs/YvsY% \textcolor{red}{python3 YvsY.py}
 Observable names:
 ['meanpt_pion', 'meanpt_kaon', 'meanpt_proton', 'Rinv', 'v2', 'RAA']
 Enter iY: 4
 39 of 49  points within 1 sigma
}
\end{Verbatim}
}\vspace*{-12pt}
The script will prompt the User for which observable to consider. In this case, choose 0-5 for the six possible observables. In this case '4' was entered and the chosen observable was the elliptic flow {\tt v2}. 
\parbox{4.5in}{\centerline{\includegraphics[width=4.0in]{figs/YvsY_rhic.pdf}}}
\parbox{2.25in}{Comparison of full-model values (black squares) for 50 points in parameter space to emulator values (red circles). The uncertainties are solely those associated with the emulation. If the uncertainties were accurately expressed, 68\% of the points would lie within the uncertainty intervals.}

For this example the full-model test data was stored in a few files in the {\tt smooth\_data/fullmodel\_testdata/} directory, consistent with the SURMISE format. Another option would be to use full-model run data stored in the same format and location as the training data. In that case one would invoke the program {\tt smoothy\_testvsfullmodel} (with the {\tt alt}) and the testing runs would be denoted by the {\tt SmoothEmulator\_TestPts} parameter set in the {\tt smooth\_data/smooth\_parameters/emulator\_parameters.txt} file. Those model runs should be chosen separately from those used for training, i.e. those denoted by the {\tt SmoothEmulator\_TrainingPts} parameter.

\end{document}
